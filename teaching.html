<!doctype html>
<html lang="en">

<head>
    <link href="./css/bootstrap.min.css" rel="stylesheet">
    <link href="./css/blog.css" rel="stylesheet">

</head>
<body>

    <div class="blog-post">
        <p class="blog-header-title"">Courses 
        <ul class="blog-list-ul">
          <li class="blog-list">
             <span>University of Indianapolis: CSCI 400-SP Applications in Natural Language Processing</span> 
          </li>
          <li class="blog-list">
               <span>University of Indianapolis: CSCI 310 Game Programming and Graphical User Interfaces</span> 
          </li>
          <li class="blog-list">
            <span>University of Indianapolis: SWEN 400 Software Project Management</span> 
         </li>
         <li class="blog-list">
            <span>University of Indianapolis: SWEN 330 Software Architecture </span> 
         </li>
           <li class="blog-list">
              <span>University of Indianapolis: SWEN 320 Software Validation and Testing</span> 
         </li>
          <li class="blog-list">
               <span>University of Indianapolis: SWEN 310 Operating Systems</span> 
          </li>
          <li class="blog-list">
            <span>University of Indianapolis: CSCI 155 Introduction to Programming Language</span> 
         </li>
          <li class="blog-list">
             <span>University of Indianapolis: CSCI 156 Introduction to Object-Oriented Programming </span> 
          </li>
          </ul>
        </div>

        <div class="blog-post">
         <p class="blog-header-title"">Development Environments</p>
         <ul class="blog-list-ul">
            <li class="blog-list"><a href="https://github.com/UIndy-CS-SWEN/vagrant_dev_environment">Vagrant VM for Ubuntu</a></li>
            <li class="blog-list"><a href="https://github.com/UIndy-CS-SWEN/docker_environments">Docker Environment for Python and Machine Learning</a></li>
            <li class="blog-list"><a href="https://github.com/UIndy-CS-SWEN/docker_lamp">Docker Environment for LAMP</a></li>
         </ul>

        <div class="blog-post">
          <p class="blog-header-title"">Selected Papers for Learning NLP</p>
          <ul class="blog-list-ul">
            <!--LLM-->
            <li class="blog-list"><a href="https://arxiv.org/pdf/2303.08774.pdf">Open AI. GPT-4 Technical Report, 2023.</a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/2303.12712.pdf">Sebastien Bubeck, et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4, 2023.</a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/2203.02155.pdf">Long Ouyang, et al. Training language models to follow instructions with human feedback, 2022.</a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/2203.02155.pdf">Yuntao Bai, et al. Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, 2022.</a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/2203.02155.pdf">Nisan Stiennon, et al. Learning to summarize from human feedback, 2022.</a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/2203.02155.pdf">Amanda Askell, et al. A General Language Assistant as a Laboratory for Alignment, 2021.</a></li>

            <!-- Machine Learning and NLP Tasks -->
            <li class="blog-list"><a href="https://arxiv.org/abs/1810.04805">Jacob Devlin, et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018.</a></li>
            <li class="blog-list"><a href="https://arxiv.org/abs/1907.11692">Yihan Liu et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2020. </a></li>
            <li class="blog-list"><a href="https://dl.acm.org/doi/10.1145/279943.279962">Avrim Blum and Tom Mitchell. Combining Labeled and Unlabeled Data with Co-Training, 1998. </a></li>
            <li class="blog-list"><a href="https://www.semanticscholar.org/paper/Conditional-Random-Fields%3A-Probabilistic-Models-for-Lafferty-McCallum/f4ba954b0412773d047dc41231c733de0c1f4926">John Lafferty, Andrew McCallum, Fernando C.N. Pereira. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, ICML 2001.</a> </li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/2005.04118.pdf">Marco Tulio Ribeiro et al. Beyond Accuracy: Behavioral Testing of NLP Models with CheckList, ACL 2020. </a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/1509.01626.pdf">Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional Networks for Text Classification, NIPS 2015. </a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/1408.5882.pdf">Yoon Kim. Convolutional Neural Networks for Sentence Classification, 2014. </a></li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/1809.04283.pdf">Shikhar Vashishth, Manik Bhandari, Prateek Yadav, Piyush Rai, Chiranjib Bhattacharyya and Partha Talukdar. Incorporating Syntactic and Semantic Information in Word Embeddings using Graph Convolutional Networks. ACL 2019</a></li>
            <!-- -->
            <li class="blog-list"> Peter F Brown, et al. Class-Based n-gram Models of Natural Language, 1992. </li>
            <li class="blog-list"> Tomas Mikolov, et al. Efficient Estimation of Word Representations in Vector Space, 2013. </li>
            <li class="blog-list"> Tomas Mikolov, et al. Distributed Representations of Words and Phrases and their Compositionality, NIPS 2013. </li>
            <li class="blog-list"><a href="https://arxiv.org/pdf/1405.4053v2.pdf"> Quoc Le, et al. Distributed Representations of Sentences and Documents. 2014.</a></li>
            <li class="blog-list"> Quoc V. Le and Tomas Mikolov: Distributed Representations of Sentences and Documents, 2014. </li>
            <li class="blog-list"> Jeffrey Pennington, et al.: GloVe: Global Vectors for Word Representation, 2014. </li>
            <!--Topic Models-->
            <li class="blog-list"><a href="https://sigir.org/wp-content/uploads/2017/06/p211.pdf">Thomas Hofmann: Probabilistic Latent Semantic Indexing, SIGIR 1999.</a> </li>
            <li class="blog-list"> David Blei, Andrew Y. Ng, and Michael I. Jordan: Latent Dirichlet Allocation, J. Machine Learning Research, 2003. </li>
            <!-- Automatic Text Summarization -->
            <li class="blog-list"> Alexander M Rush, et al.: A Neural Attention Model for Sentence Summarization. EMNLP 2015. </li>

            <!-- Question and Answering-->
            <li class="blog-list"> Minjoon Soo et al.: Bi-Directional Attention Flow for Machine Comprehension. ICLR 2015. </li>
            <li class="blog-list"> Quoc V. Le and Tomas Mikolov: Distributed Representations of Sentences and Documents, 2014. </li>
            <li class="blog-list"> Jeffrey Pennington, et al.: GloVe: Global Vectors for Word Representation, 2014. </li>


            <!-- Text Classification-->
            <li class="blog-list"> Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks. Chen Zhang, Qiuchi Li and Dawei Song.EMNLP 2019 </li>
            <li class="blog-list"> Relational Graph Attention Network for Aspect-based Sentiment Analysis. Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, Rui Wang. ACL 2020 </li>
            <li class="blog-list"> Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks. Binxuan Huang and Kathleen M. Carley.EMNLP 2019 </li>
        
    
            <!-- Knowledge Graph-->
            <li class="blog-list"> Estimating Node Importance in Knowledge Graphs Using Graph Neural Networks. Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao and Christos Faloutsos.KDD 2019 </li>
            <li class="blog-list"> Hashing Graph Convolution for Node Classification. Wenting Zhao, Zhen Cui, Chunyan Xu, Chengzheng Li, Tong Zhangï¼ŒJian Yang.CIKM 2019 </li>
           
        </ul>
          </div>





</body>
</html>